#!/bin/bash
set -e

trap 'catch $? $LINENO' ERR

# colors
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

function catch() {
  echo "Bash Script Error $1 occurred on Line $2"
  bash_error=true
  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" ]]
  then
    set +e
    upload_temp_folder
    set -e
  fi
}

function job_definitions(){
  echo "Pipeline Runner started with arguments \"$@\""

  if [[ "$RUNNER_MACHINE" = "local-dev" ]]
  then # For local dev environment
    script_dir=$(realpath "$(dirname "$BASH_SOURCE")")
    export REPO_ROOT=$(git rev-parse --show-toplevel)
    export LOCAL_DEV=true
    uuid=$(uuidgen)
    export SF_EXECUTION_NAME=Dev-$(whoami)-$uuid
    export AWS_BATCH_JOB_ID="Process-$$"
    
    source $REPO_ROOT/.getenv
    
    echo
    [[ $CLEAR_SCREEN == "true" ]] && clear
  else  # Run For local docker or remote environment
    export REPO_ROOT=${HOME}
  fi

  [[ $ECHO_COMMANDS == "true" ]] && set -x
  echo "\"$RUNNER_MACHINE\"" Environment Variables
  echo
  env | sort | grep -vE 'DIRENV|__|_|aws_access_key_id|aws_secret_access_key|AWS_ACCESS_KEY_ID|AWS_SECRET_ACCESS_KEY|OLDPWD'
  echo 
  export WORK_FOLDER=$REPO_ROOT/$WORK_FOLDER_NAME
  echo Repository folder : \"$REPO_ROOT\"
  echo Work folder : \"$WORK_FOLDER\"
  echo

  # Create temporary folders
  [ ! -d $WORK_FOLDER ] && mkdir $WORK_FOLDER_NAME
  [ -d $WORK_FOLDER/temp-job ] && rm -rf $WORK_FOLDER/temp-job
  [ ! -d $WORK_FOLDER/temp-job ] && mkdir $WORK_FOLDER_NAME/temp-job
  
  # Write env vars to log file
  env >  $WORK_FOLDER/temp-job/envvars.log
  
  export TG_COMMAND=${1:-$TG_COMMAND}
  cron_job=$2

  [ -z $SQS_MESSAGE_GROUP_ID ] && export SQS_MESSAGE_GROUP_ID=$AWS_BATCH_JOB_ID

  job_name=$WORKSPACE_ID/$SF_EXECUTION_NAME/$AWS_BATCH_JOB_ID
}

function fetch_from_gitlab(){
  echo
  echo Fetching repository from gitlab
  echo

  # Download repo from gitlab api, project version can be version tag (ex v0.1.1) or sha commit or branch name
  curl --header "PRIVATE-TOKEN: $token" \
    "https://gitlab.com/api/v4/projects/$GITLAB_PROJECT_ID/repository/archive.zip?sha=$GITLAB_PROJECT_VERSION" \
    -o downloaded_repo.zip

  # Unzip the zipped file
  unzip -o downloaded_repo.zip
  rm downloaded_repo.zip

}

function fetch_from_github(){
  echo
  echo Fetching repository from Github, Ref:$REPO_REFERENCE
  echo

  #repo_download_link=https://api.github.com/repos/${REPO_ACCOUNT}/${REPO_NAME}/tarball/${REPO_REFERENCE}
  # redirects here
  repo_download_link=https://codeload.github.com/${REPO_ACCOUNT}/${REPO_NAME}/legacy.tar.gz/refs/heads/${REPO_REFERENCE}

  [[ $REPO_TYPE == "private" ]] && token_header=--header\="Authorization:token $token" 
  
  cwd=$(pwd)
  cd $WORK_FOLDER/temp-job/
  wget $token_header --header=Accept:application/vnd.github.v3.raw -O - $repo_download_link | tar xz
  
  # Rename extracted folder name to repo name
  dir_name=$(ls -d $REPO_ACCOUNT-$REPO_NAME*)
  echo Downloaded directory name : $dir_name
  commit=$(echo $dir_name | sed s/$REPO_ACCOUNT-$REPO_NAME-//g)
  echo Commit Hash : $commit
  export COMMIT_HASH=$commit
  echo dir_name : $dir_name
  # create a clone
  
  echo Creating a clone
  
  rsync -a --exclude=scripts --exclude=library $WORK_FOLDER/temp-job/$dir_name/infra/* $WORK_FOLDER/temp-job/$WORKSPACE_ID

  library_folder=$REPO_ROOT/infra/library
  echo Copying library_folder $library_folder
  rsync -a $library_folder/* $WORK_FOLDER/temp-job/$WORKSPACE_ID/library
  
  cd $cwd
  
}

# Fetch Repo
function fetch_repository(){

  # Get secrets from envvars
  user=$(echo $REPO_ACCESS_TOKEN | jq -r .user)
  token=$(echo $REPO_ACCESS_TOKEN | jq -r .token)

  case $VCS_PROVIDER in

    gitlab )
      fetch_from_gitlab
    ;;

    github )
      fetch_from_github
    ;;

    *)
      echo "Unknown REPO_TYPE, Use One of GITHUB/GITLAB"
      exit -1
    ;;

  esac

  echo
}

function copy_local_app_repo(){
  infra_repo_link=$REPO_ROOT/local/$LOCAL_APP_REPO 
  echo Copying source files from Local $infra_repo_link folder 
  rsync -a --exclude-from=$infra_repo_link/.gitignore $infra_repo_link/* $WORK_FOLDER/temp-job/$LOCAL_APP_REPO
  # create a clone
  echo creating a clone 
  rsync -a $WORK_FOLDER/temp-job/$LOCAL_APP_REPO/infra/* $WORK_FOLDER/temp-job/$WORKSPACE_ID
  # get commit hash
  export COMMIT_HASH=$(git -C $infra_repo_link log main..$REPO_REFERENCE --format="%h" -n 1)
  echo Commit Hash : $COMMIT_HASH
  echo Repo Head : $(git -C $infra_repo_link describe --all)
  echo Changed Files : $(git -C $infra_repo_link status -s | sed s/??//g | tr -d '\n')
  if [ -h $WORK_FOLDER/temp-job/$WORKSPACE_ID/library ]
  then
    rm $WORK_FOLDER/temp-job/$WORKSPACE_ID/library
    echo copying library
    rsync -a $REPO_ROOT/infra/library/* $WORK_FOLDER/temp-job/$WORKSPACE_ID/library
  fi
}

function fetch_app_repo(){
  
  if [[ $USE_LOCAL_REPO == "true" ]]
  then 
    copy_local_app_repo
  else
    fetch_repository
  fi
  
}


# Upload Temp Folder
function upload_temp_folder(){
  # UPLOAD_WORKFOLDER never|always|onerror
  # onerror will run on Terragrunt errors and bash errors
  # never disables it - use for local dev env
  # always runs it on every execution

  if [[ $UPLOAD_WORKFOLDER == "always" ]] || [[ $UPLOAD_WORKFOLDER == "onerror" && $tg_err -ne 0 ]] || [[ ! -z $bash_error && $UPLOAD_WORKFOLDER != "never" ]]
  then
    echo Uploading temp folder
    # Find Dublicates and replace with symlinks
    # Thanks to https://github.com/pauldreik/rdfind
    cd $WORK_FOLDER
    rdfind -makesymlinks true temp-job
    # Compress Work folder
    tar -czf temp-job.tar.gz temp-job
    # Upload to S3
    aws configure set default.s3.max_concurrent_requests 10
    aws configure set default.s3.multipart_threshold 1000MB # Disable concurrency since it doesn't work in docker
    aws configure set default.s3.multipart_chunksize 100MB
    s3_file_name="$job_name/temp-job/$AWS_BATCH_JOB_ID.tar.gz"
    #aws --profile $PIPELINE_AWS_PROFILE s3 cp temp-job.tar.gz s3://$S3_JOBS_BUCKET/$s3_file_name
    aws --profile $PIPELINE_AWS_PROFILE s3api put-object --bucket $S3_JOBS_BUCKET --key $s3_file_name --body temp-job.tar.gz
    rm temp-job.tar.gz
  fi
}

# Fetch Secrets from secret manager
function fetch_secret {
  echo -e "${GREEN}- Feching Secret $1 from secret manager ${NC}";echo
  secret_value=$(aws secretsmanager get-secret-value --secret-id $2)
  echo "Successfully fetched $1";
  export $1=$(echo $secret_value | jq .SecretString | sed s/[\\]//g  | sed s/^\"//g | sed s/\}\"/\}/g )
}

# Configure aws profile
function configure_aws_profile(){
  pipeline_aws_access_key_id=$(echo ${!PIPELINE_AWS_SECRET} | jq -r .aws_access_key_id)
  pipeline_aws_secret_access_key=$(echo ${!PIPELINE_AWS_SECRET} | jq -r .aws_secret_access_key)
  
  # Default
  if [ ! $(cat $HOME/.aws/credentials | grep default) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[default]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = $PIPELINE_AWS_REGION
EOF
  fi

  # Infra Structure account
  if [ ! $(cat $HOME/.aws/credentials | grep $PIPELINE_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$PIPELINE_AWS_PROFILE]
aws_access_key_id = $pipeline_aws_access_key_id
aws_secret_access_key = $pipeline_aws_secret_access_key
region = $PIPELINE_AWS_REGION
EOF
  fi

  # Target account
  aws_access_key_id=$(echo ${!TARGET_AWS_SECRET} | jq -r .aws_access_key_id)
  aws_secret_access_key=$(echo ${!TARGET_AWS_SECRET} | jq -r .aws_secret_access_key)
  if [ ! $(cat $HOME/.aws/credentials | grep $TARGET_AWS_PROFILE) ]
  then
    cat <<EOF >> $HOME/.aws/credentials
[$TARGET_AWS_PROFILE]
aws_access_key_id = $aws_access_key_id
aws_secret_access_key = $aws_secret_access_key
region = $TARGET_AWS_REGION
EOF
  fi
}


function find_modules(){
  echo Getting Modules List...
  #tg_modules=$(python3 -m runtask json-modules)
  tg_groups=$(python3 -m runtask json-groups)
  export TG_MODULES_LIST=$(echo $tg_groups | jq 'values[][]' | jq . --slurp -c)
  echo "Terragrunt will run the modules in the following order."
  echo $tg_groups | jq .
  export TG_MODULES_COMPLETED=[]
  export TG_MODULES_COUNT=$(echo $TG_MODULES_LIST | jq length)
  echo
  echo Total number of modules to be processed: $TG_MODULES_COUNT
  echo
  sleep 1
  
}

function test_progress(){
  export TG_PARRENT_DIR=temp-job
  len=$(($TG_MODULES_COUNT - 1))
  for i in $(seq 0 0)
    do
      source terragrunt/scripts/job_complete.sh $STACK_FOLDER/$(echo $TG_MODULES_LIST | jq -r --arg i $i '.[$i|tonumber]')
    done
}

# Put outputs to S3
function put_outputs_to_s3(){

  job_resources_folder="$WORK_FOLDER/temp-job/$WORKSPACE_ID/$STACK_FOLDER/job-resources"
  # For Apply
  if [[ $TG_COMMAND == "apply" ]] && [[ $RUN_ALL == "true" ]] && [ $tg_err -eq 0 ] && [ -d $job_resources_folder ]
  then
    s3_parent_key="$S3_JOBS_BUCKET/$job_name"
    
    cd $WORK_FOLDER/temp-job/$WORKSPACE_ID/$STACK_FOLDER/job-resources
    TG_DISABLE_CONFIRM=true terragrunt output job_resources > outputs.txt 

    if [[ $UPLOAD_JOB_RESOURCES == "true" ]]
    then
      echo "Outputs for Job $job_name as text"
      cat outputs.txt
      aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.txt s3://$s3_parent_key/job-resources/outputs.txt
    fi

    echo "Outputs for Job $job_name as json"
    cat outputs.txt | jq -r '.|fromjson' > outputs.json
    cat outputs.json | jq .

    if [[ $UPLOAD_JOB_RESOURCES == "true" ]]
    then
      aws --profile $PIPELINE_AWS_PROFILE s3 cp outputs.json s3://$s3_parent_key/job-resources/outputs.json
    fi

  fi

  # For Plan Files
  if [[ $TG_COMMAND == "plan" ]] && [[ $UPLOAD_PLAN_FILES == "true" ]]
  then
    tg_working_dir=$WORK_FOLDER/temp-job/$WORKSPACE_ID
    job_log_file=$tg_working_dir/completed_tasks.log

    if [ -f $job_log_file ]
    then
      completed_tasks=$(cat $job_log_file )
    else
      completed_tasks=$(python3 -m runtask json_modules | sed s/\\\[//g | sed s/\\\]//g | sed s/\"//g | sed s/,//g)
    fi

    mkdir $tg_working_dir/plan-files

    for module_folder in $completed_tasks
    do
      for file in plan-state-file plan-file.txt plan-file.json
      do
        [ ! -d $tg_working_dir/plan-files/${module_folder//\/.} ] && mkdir $tg_working_dir/plan-files/${module_folder//\/.}
        src=$tg_working_dir/$STACK_FOLDER/$module_folder/$file
        dst=$tg_working_dir/plan-files/$module_folder
        [ -f $src ] && cp $src $dst
      done
    done

    # Upload
    echo "Uploading plan files to s3://$S3_JOBS_BUCKET/$job_name/plan-files"
    result=$(aws --profile $PIPELINE_AWS_PROFILE s3 cp $tg_working_dir/plan-files s3://$S3_JOBS_BUCKET/$job_name/plan-files --recursive)
    if [ $? -ne 0 ]
    then
      echo $result
    else
      echo Upload complete
    fi

  fi

}

function delete_temp_files(){
  rm -f *.log
}

function createSqsQueue(){ 
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then

    if [ -z $cron_job ]
    then
      sqs_queue_name=$SF_EXECUTION_NAME
      message_retention_period=600
    else
      sqs_queue_name=cron-job
      message_retention_period=600000
    fi

    if [ -z $SQS_QUEUE_URL ] || [[ $SQS_QUEUE_URL == "_" ]]
    then
      export SQS_QUEUE_URL=$(aws --profile $SQS_AWS_PROFILE sqs create-queue --queue-name $sqs_queue_name.fifo --attributes "{\"FifoQueue\": \"true\", \"ContentBasedDeduplication\": \"true\",\"MessageRetentionPeriod\":\"$message_retention_period\"}" | jq -r .QueueUrl)
    fi
    
    if [ -z "$SQS_QUEUE_URL" ]
    then
      echo "SQS_QUEUE_URL could not be evaluted."
      exit 1
    fi

    echo "SQS Messaging endpoind for the Job is $SQS_QUEUE_URL"

  fi
}

function send_sqs_status(){
  # send job start sqs message
  if [[ $SEND_SQS_MESSAGES == "true" ]]
  then
    echo "sending SQS message : $sqs_message"
    aws --profile $SQS_AWS_PROFILE sqs send-message --queue-url "$SQS_QUEUE_URL" --message-group-id "$SQS_MESSAGE_GROUP_ID" --message-body "$1"
  fi
}

function remove_event_rule(){
  set +e
  if [[ $TG_COMMAND == 'destroy' ]]
  then
    python3 -m runtask delete-event --name $EVENTBRIDGE_RULE
  fi
  set -e
}

function send_progress(){
  sqs_message="{\"message\":{\"status\":\"$1\",\"progress\":$2,\"jobId\":\"$AWS_BATCH_JOB_ID\"}}"
  send_sqs_status "$sqs_message"
}

# Run Terragrunt
function run_terragrunt(){

  # Prepare command line arguments

  if [[ $INTERACTIVE != true ]]
  then
    tg_non_interactive="--terragrunt-non-interactive"
    if [[ $TG_COMMAND == "apply" ]] || [[ $TG_COMMAND == "destroy" ]] && [[ $RUN_ALL != "true" ]] 
    then
      tg_non_interactive="$tg_non_interactive -auto-approve"
    fi
  fi

  if [[ $RUN_ALL == "true" ]] 
  then
    tg_run_all=run-all
  fi

  if [[ $TG_COMMAND == plan ]]
  then
    TG_ARGUMENTS="$TG_ARGUMENTS -out=plan-state-file"
  fi

  if [[ $COMPACT_WARNINGS == true ]]
  then
    tf_compact_warnings="-compact-warnings"
  fi

  working_dir=$WORK_FOLDER/temp-job/$WORKSPACE_ID/$STACK_FOLDER/$RUN_MODULE

  # Escape / Character
  escaped_stack_path=$(echo "$WORK_FOLDER/temp-job" | sed s/\\//\\\\\\//g)
  
  if [[ $COMPACT_STDOUT == true ]]
  then
    stderr_output=/dev/null
  else
    stderr_output=1
  fi


  
  # Prepare command line arguments for interactive or non-interactive usage
  if [[ $INTERACTIVE == "true" ]]
  then
    echo Interavtive Session Activated
  else
    export TG_DISABLE_CONFIRM=true
  fi
  
  [[ $INTERACTIVE != "true" ]] && set +e
  [[ -z $LOG_LEVEL ]] && export LOG_LEVEL=info

  # Run terragrunt init
  if  [[ $TG_COMMAND == apply ]] || [[ $TG_COMMAND == destroy ]] || [[ $TG_COMMAND == plan ]] 
  then
    if [[ $FORCE_INIT == true ]]
    then
      echo ---------- Initializing Modules ----------------------
      terragrunt $tg_run_all init \
        --terragrunt-working-dir $working_dir  \
        $tg_non_interactive --terragrunt-log-level $LOG_LEVEL $no_color $tf_compact_warnings \
        > >(tee $WORK_FOLDER/temp-job/stdout.log) \
        2> >(tee $WORK_FOLDER/temp-job/stderr.log >&$stderr_output) 
    fi
  fi

  # Run terragrunt command
  echo ------------- Running Terragrunt ----------------------
  terragrunt $tg_run_all $TG_COMMAND \
    --terragrunt-debug --terragrunt-working-dir $working_dir  \
    $tg_non_interactive --terragrunt-log-level $LOG_LEVEL $no_color $tf_compact_warnings \
    $TG_ARGUMENTS \
    > >(tee $WORK_FOLDER/temp-job/stdout.log) \
    2> >(tee $WORK_FOLDER/temp-job/stderr.log >&$stderr_output) 

  # Get Exit Code
  export tg_err=$?
  [ $tg_err -ne 0 ] && echo Terragrunt Exitcode: $tg_err

  # Prepare a consise error log
  
  cat $WORK_FOLDER/temp-job/stderr.log | grep -vE 'level=debug|locals|msg=run_cmd|msg=Detected|msg=Included|msg=\[Partial\]|msg=Executing|msg=WARN|msg=Setting|^$|msg=Generated|msg=Downloading|msg=The|msg=Are|msg=Reading|msg=Copying|msg=Debug|must wait|msg=Variables|msg=Dependency|msg=[0-9] error occurred|Cannot process|=>|msg=Stack|msg=Unable|errors occurred|sensitive|BEGIN RSA|\[0m|You may now|any changes|should now|If you ever|If you forget|- Reusing previous| \* exit status|Include this file|Terraform can guarantee|^\t\* |^ prefix' \
    | sed s/$escaped_stack_path//g > $WORK_FOLDER/temp-job/stderr_filtered.log
  

  # If error occurs write filtered error into terminal (stdout)
  if [ $tg_err -ne 0 ] && [[ $COMPACT_STDOUT == true ]]
  then
    echo -e "${RED}------- STDERR  ----------${NC}"
    cat $WORK_FOLDER/temp-job/stderr.log
    echo ----------------------------------------
    echo Filtered TF Errors
    cat $WORK_FOLDER/temp-job/stderr.log | grep 'â”‚'
    echo ----------------------------------------
  fi

  set -e

}

function log_job_resource_links(){
  s3_folder_link="https://s3.console.aws.amazon.com/s3/buckets/$S3_JOBS_BUCKET?region=$PIPELINE_AWS_REGION"
  s3_job_folder="$s3_folder_link&prefix=$job_name"
  tar_file="$s3_job_folder/temp-job/$AWS_BATCH_JOB_ID.tar.gz"
  plan_files="$s3_job_folder/plan-files"
  job_resources="$s3_job_folder/job-resources"
  echo
  echo "-------- Artifacts ----------"
  echo "- S3 Job Folder"
  echo -e "${BLUE}$s3_job_folder/${NC}"
  echo
  echo "- TAR File for $WORK_FOLDER"
  echo -e "${BLUE}$tar_file${NC}"
  echo
  echo "- Plan files"
  echo -e "${BLUE}$plan_files/${NC}"
  echo 
  echo "- Job Resources"
  echo -e "${BLUE}$job_resources/${NC}"
  echo

}

# Main Routine
job_definitions $@
echo -e "${GREEN}$job_name Job Started${NC} "
configure_aws_profile
fetch_app_repo
if [[ $1 == "bash" ]];then /bin/bash;exit $?;fi 
createSqsQueue
send_progress "Batch_Job_Started" $INITIAL_PROGRESS
remove_event_rule
delete_temp_files
find_modules
run_terragrunt
put_outputs_to_s3
send_progress "Batch_Job_Finished" $FINAL_PROGRESS
upload_temp_folder
log_job_resource_links
echo "End of job routine for $job_name";echo
exit $tg_err
